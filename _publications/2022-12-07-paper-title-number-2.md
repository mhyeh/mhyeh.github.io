---
title: "Multi-VQG: Generating Engaging Questions for Multiple Images"
collection: publications
permalink: /publication/2022-12-07-paper-title-number-2
excerpt: 'We propose a new task, Multi-VQG, which aims to generate engaging questions for multiple images. We introduce a new dataset, MVQG, which contains arround 30,000 question and image sequence pairs. We also propose both end-to-end and dual-staged models extended from VL-T5 to generate questions with story information. We evaluate our models on MVQG and show that models with explicit story information yield better results.'
date: 2022-12-07
venue: 'EMNLP'
paperurl: 'https://arxiv.org/abs/2211.07441'
citation: 'Min-Hsuan Yeh, Vicent Chen, Ting-Hao &apos;Kenneth&apos; Haung, and Lun-Wei Ku. (2022). &quot;Multi-VQG: Generating Engaging Questions for Multiple Images,&quot; to appear in Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP).'
---

<a href='https://arxiv.org/abs/2211.07441'>Download paper here</a>

We propose a new task, Multi-VQG, which aims to generate engaging questions for multiple images. We introduce a new dataset, MVQG, which contains arround 30,000 question and image sequence pairs. We also propose both end-to-end and dual-staged models extended from VL-T5 to generate questions with story information. We evaluate our models on MVQG and show that models with explicit story information yield better results.

Recommended citation: Min-Hsuan Yeh, Vicent Chen, Ting-Hao 'Kenneth' Haung, and Lun-Wei Ku. (2022). "Multi-VQG: Generating Engaging Questions for Multiple Images," to appear in Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP).